{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Aware Text Analysis\n",
    "\n",
    "Bag-of-words based decomposition technique,\n",
    "enable us to explore relationships between documents that contain the same\n",
    "mixture of individual words. This frequency of tokens based approach \n",
    "can be very effective particularly in cases where the vocabulary of\n",
    "a specific discipline or topic is sufficient to distinguish it from or relate it to other\n",
    "text.\n",
    "\n",
    "The context in which the words appear, plays a huge role in conveying meaning.\n",
    "Consider the following phrases: “she liked the smell of roses” and “she smelled like roses.”\n",
    "Using the text normalization techniques presented in previous chapters such as stopwords\n",
    "removal and lemmatization, these two utterances would have identical bag-ofwords\n",
    "vectors though they have completely different meanings.\n",
    "\n",
    "This does not mean that bag-of-words models should be completely discounted, and\n",
    "in fact, bag-of-words models are usually very useful initial models. Nonetheless,\n",
    "lower performing models can often be significantly improved with the addition of\n",
    "contextual feature extraction. One simple, yet effective approach is to augment models\n",
    "with grammars to create templates that help us target specific types of phrases,\n",
    "which capture more nuance than words alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar-Based Feature Extraction\n",
    "\n",
    "Grammatical features such as parts-of-speech enable us to encode more contextual\n",
    "information about language. One of the most effective ways of improving model performance\n",
    "is by combining grammars and parsers, which allow us to build up lightweight\n",
    "syntactic structures to directly target dynamic collections of text that could be\n",
    "significant.\n",
    "\n",
    "To get information about the language in which the sentence is written, we need a set\n",
    "of grammatical rules that specify the components of well-structured sentences in that\n",
    "language; this is what a grammar provides. A grammar is a set of rules describing\n",
    "specifically how syntactic units (sentences, phrases, etc.) in a given language should\n",
    "be deconstructed into their constituent units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T08:54:27.219483Z",
     "start_time": "2020-07-04T08:54:27.208514Z"
    }
   },
   "source": [
    "### Context-Free Grammars\n",
    "We can use grammars to specify different rules that allow us to build up parts-of-speech\n",
    "into phrases or chunks. A context-free grammar is a set of rules for combining\n",
    "syntactic components to form sensical strings. For instance, the noun phrase “the castle”\n",
    "has a determiner (DT) and a noun (N).\n",
    "The prepositional phrase (PP) “in the castle” has a preposition (P) and a noun phrase\n",
    "(NP). The verb phrase (VP) “looks in the castle” has a verb (V) and a prepositional\n",
    "phrase (PP). The sentence (S) “Gwen looks in the castle” has a proper noun (NNP) and\n",
    "verb phrase (VP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T12:01:14.046258Z",
     "start_time": "2020-07-04T12:01:14.041282Z"
    }
   },
   "outputs": [],
   "source": [
    "GRAMMAR = \"\"\"\n",
    "S -> NNP VP\n",
    "VP -> V PP\n",
    "PP -> P NP\n",
    "NP -> DT N\n",
    "NNP -> 'Gwen' | 'George'\n",
    "V -> 'looks' | 'burns'\n",
    "P -> 'in' | 'for'\n",
    "DT -> 'the'\n",
    "N -> 'castle' | 'ocean'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T12:01:15.573583Z",
     "start_time": "2020-07-04T12:01:14.049249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 13 productions (start state = S)\n",
      "    S -> NNP VP\n",
      "    VP -> V PP\n",
      "    PP -> P NP\n",
      "    NP -> DT N\n",
      "    NNP -> 'Gwen'\n",
      "    NNP -> 'George'\n",
      "    V -> 'looks'\n",
      "    V -> 'burns'\n",
      "    P -> 'in'\n",
      "    P -> 'for'\n",
      "    DT -> 'the'\n",
      "    N -> 'castle'\n",
      "    N -> 'ocean'\n",
      "S\n",
      "[S -> NNP VP, VP -> V PP, PP -> P NP, NP -> DT N, NNP -> 'Gwen', NNP -> 'George', V -> 'looks', V -> 'burns', P -> 'in', P -> 'for', DT -> 'the', N -> 'castle', N -> 'ocean']\n"
     ]
    }
   ],
   "source": [
    "from nltk import CFG\n",
    "cfg = CFG.fromstring(GRAMMAR)\n",
    "print(cfg)\n",
    "print(cfg.start())\n",
    "print(cfg.productions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic Parsers\n",
    "\n",
    "Once we have defined a grammar, we need a mechanism to systematically search out\n",
    "the meaningful syntactic structures from our corpus; this is the role of the parser. If a\n",
    "grammar defines the search criterion for “meaningfulness” in the context of our language,\n",
    "the parser executes the search. A syntactic parser is a program that deconstructs\n",
    "sentences into a parse tree, which consists of hierarchical constituents, or\n",
    "syntactic categories.\n",
    "\n",
    "When a parser encounters a sentence, it checks to see if the structure of that sentence\n",
    "conforms to a known grammar. If so, it parses the sentence according to the rules of\n",
    "that grammar, producing a parse tree. Parsers are often used to identify important\n",
    "structures, like the subject and object of verbs in a sentence, or to determine which\n",
    "sequences of words in a sentence should be grouped together within each syntactic\n",
    "category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T12:01:15.585552Z",
     "start_time": "2020-07-04T12:01:15.577572Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.chunk.regexp import RegexpParser\n",
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "chunker = RegexpParser(GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T12:01:15.737067Z",
     "start_time": "2020-07-04T12:01:15.589539Z"
    }
   },
   "outputs": [],
   "source": [
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "GOODTAGS = frozenset(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from unicodedata import category as unicat\n",
    "from itertools import groupby\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk import pos_tag\n",
    "\n",
    "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wraps a PickledCorpusReader consisting of pos-tagged documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, grammar=GRAMMAR):\n",
    "        self.grammar = GRAMMAR\n",
    "        self.chunker = RegexpParser(self.grammar)\n",
    "        \n",
    "        \n",
    "    def normalize(self, sent):\n",
    "        \"\"\"\n",
    "        Removes punctuation from a tokenized/tagged sentence and\n",
    "        lowercases words.\n",
    "        \"\"\"\n",
    "        sent = pos_tag(sent.split())\n",
    "        is_punct = lambda word: all(unicat(c).startswith('P') for c in word)\n",
    "        sent = filter(lambda t: not is_punct(t[0]), sent)\n",
    "        sent = map(lambda t: (t[0].lower(), t[1]), sent)\n",
    "        return list(sent)      \n",
    "    \n",
    "    def extract_keyphrases(self, sent):\n",
    "        \"\"\"\n",
    "        For a document, parse sentences using our chunker created by\n",
    "        our grammar, converting the parse tree into a tagged sequence.\n",
    "        Yields extracted phrases.\n",
    "        \"\"\"\n",
    "        sent = self.normalize(sent)\n",
    "        chunks = tree2conlltags(self.chunker.parse(sent))\n",
    "        phrases = [\n",
    "            \" \".join(word for word, pos, chunk in group).lower()\n",
    "            for key, group in groupby(\n",
    "                chunks, lambda term: term[-1] != 'O'\n",
    "            ) if key\n",
    "        ]\n",
    "        for phrase in phrases:\n",
    "            yield phrase\n",
    "            \n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.extract_keyphrases(document)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T12:01:16.038514Z",
     "start_time": "2020-07-04T12:01:15.740064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n",
      "['beautiful view']\n",
      "['wonderful sight']\n"
     ]
    }
   ],
   "source": [
    "kpe = KeyphraseExtractor()\n",
    "kps = kpe.fit_transform(\n",
    "        ['Hello how are you', 'Such a beautiful view', 'What a wOnderfUl sight'],\n",
    ")\n",
    "\n",
    "for kp in kps:\n",
    "    print(list(kp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T12:01:16.073422Z",
     "start_time": "2020-07-04T12:01:16.050483Z"
    }
   },
   "outputs": [],
   "source": [
    "GOODLABELS = frozenset(['PERSON', 'ORGANIZATION', 'FACILITY', 'GPE', 'GSP'])\n",
    "from nltk import ne_chunk\n",
    "\n",
    "class EntityExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, labels=GOODLABELS, **kwargs):\n",
    "        self.labels = labels\n",
    "    \n",
    "    def get_entities(self, sentence):\n",
    "        entities = []\n",
    "        trees = ne_chunk(pos_tag(sentence.split()))\n",
    "        for tree in trees:\n",
    "            if hasattr(tree, 'label'):\n",
    "                if tree.label() in self.labels:\n",
    "                    entities.append(\n",
    "                        ' '.join([child[0].lower() for child in tree])\n",
    "                    )\n",
    "        return entities\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.get_entities(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T12:01:16.379913Z",
     "start_time": "2020-07-04T12:01:16.079405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barak', 'obama', 'united states']\n"
     ]
    }
   ],
   "source": [
    "ee = EntityExtractor()\n",
    "\n",
    "entities = ee.fit_transform(['Barak Obama is the President of United States'])\n",
    "\n",
    "for entity in entities:\n",
    "    print(list(entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-Gram Feature Extraction\n",
    "\n",
    "Grammar-based approaches, while very effective, do not always work.\n",
    "For one thing, they rely heavily on the success of part-of-speech tagging, meaning we\n",
    "must be confident that our tagger is correctly labeling nouns, verbs, adjectives, and\n",
    "other parts of speech.\n",
    "\n",
    "Grammar-based feature extraction is also somewhat inflexible, because we must\n",
    "begin by defining a grammar. It is often very difficult to know in advance which\n",
    "grammar pattern will most effectively capture the high-signal terms and phrases\n",
    "within a text.\n",
    "We can address these challenges iteratively, by experimenting with many different\n",
    "grammars or by training our own custom part-of-speech tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing the Right n-Gram Window**\n",
    "\n",
    "Choosing n can also be considered as balancing the trade-off between bias and variance.\n",
    "A small n leads to a simpler (weaker) model, therefore causing more error due\n",
    "to bias. A larger n leads to a more complex model (a higher-order model), thus causing\n",
    "more error due to variance. Just as with all supervised machine learning problems,\n",
    "we have to strike the right balance between the sensitivity and the specificity of\n",
    "our model. The more dependent words are on more distant precursors, the greater\n",
    "the complexity needed for an n-gram model to be predictive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Significant Collocations**\n",
    "\n",
    "Using raw n-grams will produce many,\n",
    "many candidates, most of which will not be relevant. For example, the sentence “I got\n",
    "lost in the corn maze during the fall picnic” contains the trigram ('in', 'the',\n",
    "'corn'), which is not a typical prepositional target, whereas the trigram ('I',\n",
    "'got', 'lost') seems to make sense on its own.\n",
    "\n",
    "In practice, this is too high a computational cost to be useful in most applications.\n",
    "The solution is to compute conditional probability. For example, what is the likelihood\n",
    "that the tokens ('the', 'fall') appear in the text given the token 'during'? We\n",
    "can compute empirical likelihoods by calculating the frequency of the (n-1)-gram\n",
    "conditioned by the first token of the n-gram. Using this technique we can value ngrams\n",
    "that are more often used together such as ('corn', 'maze') over rarer compositions\n",
    "that are less meaningful.\n",
    "\n",
    "The idea of some n-grams having more value than others leads to another tool in the\n",
    "text analysis toolkit: significant collocations. Collocation is an abstract synonym for ngram\n",
    "(without the specificity of the window size) and simply means a sequence of\n",
    "tokens whose likelihood of co-occurrence is caused by something other than random\n",
    "chance. Using conditional probability, we can test the hypothesis that a specified collocation\n",
    "is meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![ngram](../meta/ngram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-Gram Language Model\n",
    "\n",
    "ngram models utilize the statistical frequency of n-grams to make decisions about text.\n",
    "To compute an n-gram language model that predicts the next word after a series of\n",
    "words, we would first count all n-grams in the text and then use those frequencies to\n",
    "predict the likelihood of the last token in the n-gram given the tokens that precede it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
