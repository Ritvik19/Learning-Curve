{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:26.466055Z",
     "start_time": "2020-06-30T06:00:23.190311Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy \n",
    "from spacy.matcher import Matcher \n",
    "\n",
    "import visualise_spacy_tree\n",
    "from IPython.display import Image, display\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:27.187708Z",
     "start_time": "2020-06-30T06:00:26.468049Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Ritvik\\Anaconda3\\envs\\ailab\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction\n",
    "\n",
    "The task of Information Extraction (IE) involves extracting meaningful information from unstructured text data and presenting it in a structured format.\n",
    "\n",
    "* Reference: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/06/nlp-project-information-extraction/) \n",
    "* Supplimentary: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2019/09/introduction-information-extraction-python-spacy/?utm_source=blog&utm_medium=nlp-project-information-extraction)\n",
    "* NLP Learning Path: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/01/learning-path-nlp-2020/?utm_source=blog&utm_medium=nlp-project-information-extraction)\n",
    "\n",
    "Information Extraction (IE) is a crucial cog in the field of Natural Language Processing (NLP) and linguistics. It’s widely used for tasks such as Question Answering Systems, Machine Translation, Entity Extraction, Event Extraction, Named Entity Linking, Coreference Resolution, Relation Extraction, etc.\n",
    "\n",
    "In information extraction, there is an important concept of triples.\n",
    "\n",
    "> A triple represents a couple of entities and a relation between them. For example, (Obama, born, Hawaii) is a triple in which ‘Obama’ and ‘Hawaii’ are the related entities, and the relation between them is ‘born’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:50:41.261474Z",
     "start_time": "2020-06-29T17:50:41.254492Z"
    }
   },
   "source": [
    "## Different approaches to Information Extraction\n",
    "\n",
    "![ietypes](../meta/IE_types.webp)\n",
    "\n",
    "* In Traditional Information Extraction, the relations to be extracted are pre-defined\n",
    "\n",
    "* In Open Information Extraction, the relations are not pre-defined. The system is free to extract any relations it comes across while going through the text data.\n",
    "\n",
    "### Different Approaches to Traditional Information Extraction\n",
    "\n",
    "* Rule-based Approach: We define a set of rules for the syntax and other grammatical properties of a natural language and then use these rules to extract information from text\n",
    "* Supervised: Let’s say we have a sentence S. It has two entities E1 and E2. Now, the supervised machine learning model has to detect whether there is any relation (R) between E1 and E2. So, in a supervised approach, the task of relation extraction turns into the task of relation detection. The only drawback of this approach is that it needs a lot of labeled data to train a model\n",
    "* Semi-supervised: When we don’t have enough labeled data, we can use a set of seed examples (triples) to formulate high-precision patterns that can be used to extract more relations from the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction using spaCy\n",
    "\n",
    "\n",
    "We all know that sentences are made up of words belonging to different Parts of Speech (POS). There are eight different POS in the English language: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and intersection.\n",
    "\n",
    "The POS determines how a specific word functions in meaning in a given sentence. For example, take the word “right”. In the sentence, “The boy was awarded chocolate for giving the right answer”, “right” is used as an adjective. Whereas, in the sentence, “You have the right to say whatever you want”, “right” is treated as a noun.\n",
    "\n",
    "This goes to show that the POS tag of a word carries a lot of significance when it comes to understanding the meaning of a sentence. And we can leverage it to extract meaningful information from our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:27.227114Z",
     "start_time": "2020-06-30T06:00:27.191698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This -> DET\n",
      "is -> AUX\n",
      "a -> DET\n",
      "sample -> NOUN\n",
      "sentence -> NOUN\n",
      ". -> PUNCT\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample sentence.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,'->',token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we wanted to extract nouns from the sentences, we could take a look at POS tags of the words/tokens in the sentence, using the attribute .pos_, and extract them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:27.499903Z",
     "start_time": "2020-06-30T06:00:27.231104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "sentence\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was that easy to extract words based on their POS tags. But sometimes extracting information purely based on the POS tags is not enough. Have a look at the sentence below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:27.817661Z",
     "start_time": "2020-06-30T06:00:27.502895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The -> DET\n",
      "children -> NOUN\n",
      "love -> VERB\n",
      "cream -> NOUN\n",
      "biscuits -> NOUN\n"
     ]
    }
   ],
   "source": [
    "text = \"The children love cream biscuits\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,'->',token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I wanted to extract the subject and the object from a sentence, I can’t do that based on their POS tags. For that, I need to look at how these words are related to each other. These are called Dependencies.\n",
    "\n",
    "We can make use of spaCy’s displacy visualizer that displays the word dependencies in a graphical manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:28.182660Z",
     "start_time": "2020-06-30T06:00:27.820652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"0a44d5ef05534664b17791a1dc8350af-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">children</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">love</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">cream</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">biscuits</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0a44d5ef05534664b17791a1dc8350af-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0a44d5ef05534664b17791a1dc8350af-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0a44d5ef05534664b17791a1dc8350af-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0a44d5ef05534664b17791a1dc8350af-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0a44d5ef05534664b17791a1dc8350af-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0a44d5ef05534664b17791a1dc8350af-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0a44d5ef05534664b17791a1dc8350af-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0a44d5ef05534664b17791a1dc8350af-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"The children love cream biscuits\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "displacy.render(doc, style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This directed graph is known as a dependency graph. It represents the relations between different words of a sentence.\n",
    "\n",
    "Each word is a node in the Dependency graph. The relationship between words is denoted by the edges. For example, “The” is a determiner here, “children” is the subject of the sentence, “biscuits” is the object of the sentence, and “cream” is a compound word that gives us more information about the object.\n",
    "\n",
    "The arrows carry a lot of significance here:\n",
    "\n",
    "* The arrowhead points to the words that are dependent on the word pointed by the origin of the arrow\n",
    "* The former is referred to as the child node of the latter. For example, “children” is the child node of “love”\n",
    "* The word which has no incoming arrow is called the root node of the sentence\n",
    "\n",
    "Like we have an attribute for POS in SpaCy tokens, we similarly have an attribute for extracting the dependency of a token denoted by dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:28.514851Z",
     "start_time": "2020-06-30T06:00:28.186652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects:\n",
      "children\n",
      "\n",
      "Objects:\n",
      "biscuits\n"
     ]
    }
   ],
   "source": [
    "print('Subjects:')        \n",
    "print(*[token.text for token in doc if token.dep_ == 'nsubj'])\n",
    "print()\n",
    "print('Objects:')        \n",
    "print(*[token.text for token in doc if token.dep_ == 'dobj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using POS tags and Dependency tags, we can look for relationships between different entities in a sentence. For example, in the sentence “The cat perches on the window sill“, we have the subject, “cat”, the object “window sill”, related by the preposition “on”. We can look for such relationships and much more to extract meaningful information from our text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  United Nations General Debate Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:28.990373Z",
     "start_time": "2020-06-30T06:00:28.519842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Session</th>\n",
       "      <th>Year</th>\n",
       "      <th>Country</th>\n",
       "      <th>Speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>IND</td>\n",
       "      <td>40.\\t Mr. President, I offer you our congratul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>1971</td>\n",
       "      <td>IND</td>\n",
       "      <td>38.\\tMr. President, on behalf of the people of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>1972</td>\n",
       "      <td>IND</td>\n",
       "      <td>Mr. President, I offer you on behalf of India ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>1973</td>\n",
       "      <td>IND</td>\n",
       "      <td>﻿122.\\tMr. President, I bring to you and to al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>1974</td>\n",
       "      <td>IND</td>\n",
       "      <td>Mr. President, I have already had occasion to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Session  Year Country                                             Speech\n",
       "0      25  1970     IND  40.\\t Mr. President, I offer you our congratul...\n",
       "1      26  1971     IND  38.\\tMr. President, on behalf of the people of...\n",
       "2      27  1972     IND  Mr. President, I offer you on behalf of India ...\n",
       "3      28  1973     IND  ﻿122.\\tMr. President, I bring to you and to al...\n",
       "4      29  1974     IND  Mr. President, I have already had occasion to ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = glob.glob('UNGDC-1970-2018/Converted sessions/Session*')\n",
    "\n",
    "df = pd.DataFrame(columns={'Country','Speech','Session','Year'})\n",
    "i = 0 \n",
    "for file in folders:\n",
    "    speech = glob.glob(file+'/IND*.txt')\n",
    "    with open(speech[0],encoding='utf8') as f:\n",
    "        df.loc[i,'Speech'] = f.read()\n",
    "        df.loc[i,'Year'] = speech[0].split('_')[-1].split('.')[0]\n",
    "        df.loc[i,'Session'] = speech[0].split('_')[-2]\n",
    "        df.loc[i,'Country'] = speech[0].split('_')[0].split(\"\\\\\")[-1]\n",
    "        i += 1 \n",
    "        \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:29.277505Z",
     "start_time": "2020-06-30T06:00:28.994363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Session</th>\n",
       "      <th>Year</th>\n",
       "      <th>Country</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Speech_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>IND</td>\n",
       "      <td>40.\\t Mr. President, I offer you our congratul...</td>\n",
       "      <td>Mr President, I offer you our congratulations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>1971</td>\n",
       "      <td>IND</td>\n",
       "      <td>38.\\tMr. President, on behalf of the people of...</td>\n",
       "      <td>Mr President, on behalf of the people of India...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>1972</td>\n",
       "      <td>IND</td>\n",
       "      <td>Mr. President, I offer you on behalf of India ...</td>\n",
       "      <td>Mr President, I offer you on behalf of India o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>1973</td>\n",
       "      <td>IND</td>\n",
       "      <td>﻿122.\\tMr. President, I bring to you and to al...</td>\n",
       "      <td>﻿Mr President, I bring to you and to all our c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>1974</td>\n",
       "      <td>IND</td>\n",
       "      <td>Mr. President, I have already had occasion to ...</td>\n",
       "      <td>Mr President, I have already had occasion to c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Session  Year Country                                             Speech  \\\n",
       "0      25  1970     IND  40.\\t Mr. President, I offer you our congratul...   \n",
       "1      26  1971     IND  38.\\tMr. President, on behalf of the people of...   \n",
       "2      27  1972     IND  Mr. President, I offer you on behalf of India ...   \n",
       "3      28  1973     IND  ﻿122.\\tMr. President, I bring to you and to al...   \n",
       "4      29  1974     IND  Mr. President, I have already had occasion to ...   \n",
       "\n",
       "                                        Speech_clean  \n",
       "0   Mr President, I offer you our congratulations...  \n",
       "1  Mr President, on behalf of the people of India...  \n",
       "2  Mr President, I offer you on behalf of India o...  \n",
       "3  ﻿Mr President, I bring to you and to all our c...  \n",
       "4  Mr President, I have already had occasion to c...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    \n",
    "    text = re.sub('[0-9]+.\\t','',str(text))\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    text = re.sub(\"'s\",'',str(text))\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"— \",'',str(text))\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
    "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['Speech_clean'] = df['Speech'].apply(clean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:29.478357Z",
     "start_time": "2020-06-30T06:00:29.280497Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentences(text):\n",
    "    # split sentences and questions\n",
    "    text = re.split('[.?]', text)\n",
    "    clean_sent = []\n",
    "    for sent in text:\n",
    "        clean_sent.append(sent)\n",
    "    return clean_sent\n",
    "\n",
    "df['sent'] = df['Speech_clean'].apply(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:29.834532Z",
     "start_time": "2020-06-30T06:00:29.481324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Sent</th>\n",
       "      <th>Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970</td>\n",
       "      <td>Mr President, I offer you our congratulations...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>You represent Norway, a country which can tak...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970</td>\n",
       "      <td>Your personal qualifications and your family ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970</td>\n",
       "      <td>I should also like to express our appreciatio...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970</td>\n",
       "      <td>I would also repeat our admiration for U Than...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year                                               Sent  Len\n",
       "0  1970   Mr President, I offer you our congratulations...   21\n",
       "1  1970   You represent Norway, a country which can tak...   17\n",
       "2  1970   Your personal qualifications and your family ...   13\n",
       "3  1970   I should also like to express our appreciatio...   19\n",
       "4  1970   I would also repeat our admiration for U Than...   18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(columns=['Sent','Year','Len'])\n",
    "\n",
    "row_list = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    for sent in df.loc[i,'sent']:\n",
    "    \n",
    "        wordcount = len(sent.split())\n",
    "        year = df.loc[i,'Year']\n",
    "\n",
    "        dict1 = {'Year':year,'Sent':sent,'Len':wordcount}\n",
    "        row_list.append(dict1)\n",
    "    \n",
    "df2 = pd.DataFrame(row_list)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction using SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule on Noun-Verb-Noun Phrases\n",
    "\n",
    "When you look at a sentence, it generally contains a subject (noun), action (verb), and an object (noun). The rest of the words are just there to give us additional information about the entities. Therefore, we can leverage this basic structure to extract the main bits of information from the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:29.877917Z",
     "start_time": "2020-06-30T06:00:29.837521Z"
    }
   },
   "outputs": [],
   "source": [
    "def rule1(text):\n",
    "    doc = nlp(text)\n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if (token.pos_=='VERB'):\n",
    "            phrase = ''\n",
    "            for sub_tok in token.lefts:\n",
    "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
    "                    phrase += sub_tok.text\n",
    "                    phrase += ' '+token.lemma_ \n",
    "                    for sub_tok in token.rights:\n",
    "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):       \n",
    "                            phrase += ' '+sub_tok.text\n",
    "                            sent.append(phrase)\n",
    "            \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:30.245063Z",
     "start_time": "2020-06-30T06:00:29.880912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Industrialized countries with planned economies, which do not formally belong to the international monetary system but participate in the global activities of commerce and technological exchange, also face problems of production and renovation -> ['countries face problems']\n",
      "\n",
      "\n",
      "\n",
      " The entry of these two countries into the United Nations has taken this Organization one step closer to its goal of universality -> ['entry take Organization']\n",
      "\n",
      "\n",
      "\n",
      " The two United Nations Development Decades, one of the 1960s and the other of the 1970s, and a series of protracted negotiations, have proved sterile exercises, belying the hopes that had been raised that inequity between nations need not be an inexorable law and that, for reasons as much economic as ethical, the rich should assist the poor -> ['Decades prove exercises']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in df2['Sent'].sample(10, random_state=19).values:\n",
    "    rule = rule1(sent)\n",
    "    if rule != []:\n",
    "        print(sent, '->', rule)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule on Adjective Noun Structure\n",
    "\n",
    "In the previous rule that, the information did not feel complete. This is because many nouns have an adjective or a word with a compound dependency that augments the meaning of a noun. Extracting these along with the noun will give us better information about the subject and the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:30.262018Z",
     "start_time": "2020-06-30T06:00:30.249054Z"
    }
   },
   "outputs": [],
   "source": [
    "def rule2(text):\n",
    "    doc = nlp(text)\n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        phrase = ''\n",
    "        if (token.pos_ == 'NOUN') and (token.dep_ in ['dobj','pobj','nsubj','nsubjpass']):\n",
    "            for subtoken in token.children:\n",
    "                if (subtoken.pos_ == 'ADJ') or (subtoken.dep_ == 'compound'):\n",
    "                    phrase += subtoken.text + ' '\n",
    "            if len(phrase)!=0:\n",
    "                phrase += token.text \n",
    "        if  len(phrase)!=0:\n",
    "            sent.append(phrase)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:30.665064Z",
     "start_time": "2020-06-30T06:00:30.268004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There is renewed awareness of the continued relevance of his message of non violence and tolerance -> ['non violence']\n",
      "\n",
      "\n",
      "\n",
      " Industrialized countries with planned economies, which do not formally belong to the international monetary system but participate in the global activities of commerce and technological exchange, also face problems of production and renovation -> ['Industrialized countries', 'international monetary system', 'global activities']\n",
      "\n",
      "\n",
      "\n",
      " There can be no durable peace in West Asia without a just and comprehensive settlement, based on the realization by the Palestinian people of their inalienable right to self determination and the recognition of the rights of all States in the region, including Palestine and Israel, to live in peace and security within internationally recognized borders -> ['just settlement', 'Palestinian people', 'inalienable right', 'self determination']\n",
      "\n",
      "\n",
      "\n",
      " The bulk of the global military expenditure of $1 trillion a year is accounted for by a handful of industrialized countries -> ['global military expenditure']\n",
      "\n",
      "\n",
      "\n",
      " We are now on the threshold of the third United Nations Development Decade, covering the 1980s, and of the special session of the United Nations which will be held next year -> ['special session']\n",
      "\n",
      "\n",
      "\n",
      " The two United Nations Development Decades, one of the 1960s and the other of the 1970s, and a series of protracted negotiations, have proved sterile exercises, belying the hopes that had been raised that inequity between nations need not be an inexorable law and that, for reasons as much economic as ethical, the rich should assist the poor -> ['protracted negotiations', 'sterile exercises', 'economic reasons']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in df2['Sent'].sample(10, random_state=19).values:\n",
    "    rule = rule2(sent)\n",
    "    if rule != []:\n",
    "        print(sent, '->', rule)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T16:32:45.860085Z",
     "start_time": "2020-06-29T16:32:45.844423Z"
    }
   },
   "source": [
    "## Combining both rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:30.930718Z",
     "start_time": "2020-06-30T06:00:30.668055Z"
    }
   },
   "outputs": [],
   "source": [
    "def rule2_mod(text,index):\n",
    "    doc = nlp(text)\n",
    "    phrase = ''\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.i == index:\n",
    "            for subtoken in token.children:\n",
    "                if (subtoken.pos_ == 'ADJ'):\n",
    "                    phrase += ' '+subtoken.text\n",
    "            break\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "def rule1_mod(text):\n",
    "    doc = nlp(text)\n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if (token.pos_=='VERB'):\n",
    "            phrase =''\n",
    "            for sub_tok in token.lefts:\n",
    "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
    "                    adj = rule2_mod(text,sub_tok.i)\n",
    "                    phrase += adj + ' ' + sub_tok.text\n",
    "                    phrase += ' '+token.lemma_ \n",
    "                    for sub_tok in token.rights:\n",
    "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
    "                            adj = rule2_mod(text,sub_tok.i)\n",
    "                            phrase += adj+' '+sub_tok.text\n",
    "                            sent.append(phrase)\n",
    "            \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:31.569744Z",
     "start_time": "2020-06-30T06:00:30.934708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Industrialized countries with planned economies, which do not formally belong to the international monetary system but participate in the global activities of commerce and technological exchange, also face problems of production and renovation -> [' countries face problems']\n",
      "\n",
      "\n",
      "\n",
      " The entry of these two countries into the United Nations has taken this Organization one step closer to its goal of universality -> [' entry take Organization']\n",
      "\n",
      "\n",
      "\n",
      " The two United Nations Development Decades, one of the 1960s and the other of the 1970s, and a series of protracted negotiations, have proved sterile exercises, belying the hopes that had been raised that inequity between nations need not be an inexorable law and that, for reasons as much economic as ethical, the rich should assist the poor -> [' Decades prove sterile exercises']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in df2['Sent'].sample(10, random_state=19).values:\n",
    "    rule = rule1_mod(sent)\n",
    "    if rule != []:\n",
    "        print(sent, '->', rule)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule on Prepositions\n",
    "\n",
    "Prepositions tell us where or when something is in a relationship with something else. For example, The people of India believe in the principles of the United Nations. Clearly extracting phrases including prepositions will give us a lot of information from the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:31.590688Z",
     "start_time": "2020-06-30T06:00:31.573733Z"
    }
   },
   "outputs": [],
   "source": [
    "def rule3(text):\n",
    "    doc = nlp(text)\n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_=='ADP':\n",
    "            phrase = ''\n",
    "            if token.head.pos_=='NOUN':\n",
    "                phrase += token.head.text\n",
    "                phrase += ' '+token.text\n",
    "                for right_tok in token.rights:\n",
    "                    if (right_tok.pos_ in ['NOUN','PROPN']):\n",
    "                        phrase += ' '+right_tok.text\n",
    "                \n",
    "                if len(phrase)>2:\n",
    "                    sent.append(phrase)\n",
    "                \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:31.873300Z",
     "start_time": "2020-06-30T06:00:31.595675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There is renewed awareness of the continued relevance of his message of non violence and tolerance -> ['awareness of relevance', 'relevance of message', 'message of violence']\n",
      "\n",
      "\n",
      "\n",
      " Industrialized countries with planned economies, which do not formally belong to the international monetary system but participate in the global activities of commerce and technological exchange, also face problems of production and renovation -> ['countries with economies', 'activities of exchange', 'problems of production']\n",
      "\n",
      "\n",
      "\n",
      " The entry of these two countries into the United Nations has taken this Organization one step closer to its goal of universality -> ['entry of countries', 'entry into Nations', 'goal of universality']\n",
      "\n",
      "\n",
      "\n",
      " There can be no durable peace in West Asia without a just and comprehensive settlement, based on the realization by the Palestinian people of their inalienable right to self determination and the recognition of the rights of all States in the region, including Palestine and Israel, to live in peace and security within internationally recognized borders -> ['peace in Asia', 'peace without settlement', 'realization by people', 'people of right determination', 'self to', 'recognition of rights', 'rights of States']\n",
      "\n",
      "\n",
      "\n",
      " The bulk of the global military expenditure of $1 trillion a year is accounted for by a handful of industrialized countries -> ['bulk of expenditure', 'expenditure of', 'handful of countries']\n",
      "\n",
      "\n",
      "\n",
      " He also referred to the resolutions of the Security Council of 1948 and 1949 -> ['resolutions of Council']\n",
      "\n",
      "\n",
      "\n",
      " We are now on the threshold of the third United Nations Development Decade, covering the 1980s, and of the special session of the United Nations which will be held next year -> ['threshold of Decade', 'session of Nations']\n",
      "\n",
      "\n",
      "\n",
      " The two United Nations Development Decades, one of the 1960s and the other of the 1970s, and a series of protracted negotiations, have proved sterile exercises, belying the hopes that had been raised that inequity between nations need not be an inexorable law and that, for reasons as much economic as ethical, the rich should assist the poor -> ['series of negotiations', 'inequity between nations']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in df2['Sent'].sample(10, random_state=19).values:\n",
    "    rule = rule3(sent)\n",
    "    if rule != []:\n",
    "        print(sent, '->', rule)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:32.177409Z",
     "start_time": "2020-06-30T06:00:31.875292Z"
    }
   },
   "outputs": [],
   "source": [
    "def rule0(text, index):\n",
    "    doc = nlp(text)  \n",
    "    token = doc[index]\n",
    "    entity = ''\n",
    "    \n",
    "    for sub_tok in token.children:\n",
    "        if (sub_tok.dep_ in ['compound','amod']):\n",
    "            entity += sub_tok.text+' '\n",
    "    \n",
    "    entity += token.text\n",
    "    return entity\n",
    "\n",
    "def rule3_mod(text):\n",
    "    doc = nlp(text)\n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_=='ADP':\n",
    "            phrase = ''\n",
    "            if token.head.pos_=='NOUN':\n",
    "                append = rule0(text, token.head.i)\n",
    "                if len(append)!=0:\n",
    "                    phrase += append\n",
    "                else:  \n",
    "                    phrase += token.head.text\n",
    "                phrase += ' '+token.text\n",
    "\n",
    "                for right_tok in token.rights:\n",
    "                    if (right_tok.pos_ in ['NOUN','PROPN']):\n",
    "                        right_phrase = ''\n",
    "                        append = rule0(text, right_tok.i)\n",
    "                        if len(append)!=0:\n",
    "                            right_phrase += ' '+append\n",
    "                        else:\n",
    "                            right_phrase += ' '+right_tok.text \n",
    "                        phrase += right_phrase\n",
    "                \n",
    "                if len(phrase)>2:\n",
    "                    sent.append(phrase)\n",
    "                \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T06:00:33.341333Z",
     "start_time": "2020-06-30T06:00:32.180409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There is renewed awareness of the continued relevance of his message of non violence and tolerance -> ['renewed awareness of continued relevance', 'continued relevance of message', 'message of non violence']\n",
      "\n",
      "\n",
      "\n",
      " Industrialized countries with planned economies, which do not formally belong to the international monetary system but participate in the global activities of commerce and technological exchange, also face problems of production and renovation -> ['Industrialized countries with planned economies', 'global activities of exchange', 'problems of production']\n",
      "\n",
      "\n",
      "\n",
      " The entry of these two countries into the United Nations has taken this Organization one step closer to its goal of universality -> ['entry of countries', 'entry into United Nations', 'goal of universality']\n",
      "\n",
      "\n",
      "\n",
      " There can be no durable peace in West Asia without a just and comprehensive settlement, based on the realization by the Palestinian people of their inalienable right to self determination and the recognition of the rights of all States in the region, including Palestine and Israel, to live in peace and security within internationally recognized borders -> ['durable peace in West Asia', 'durable peace without just settlement', 'realization by Palestinian people', 'Palestinian people of inalienable right self determination', 'self to', 'recognition of rights', 'rights of States']\n",
      "\n",
      "\n",
      "\n",
      " The bulk of the global military expenditure of $1 trillion a year is accounted for by a handful of industrialized countries -> ['bulk of global military expenditure', 'global military expenditure of', 'handful of industrialized countries']\n",
      "\n",
      "\n",
      "\n",
      " He also referred to the resolutions of the Security Council of 1948 and 1949 -> ['resolutions of Security Council']\n",
      "\n",
      "\n",
      "\n",
      " We are now on the threshold of the third United Nations Development Decade, covering the 1980s, and of the special session of the United Nations which will be held next year -> ['threshold of third Nations Development Decade', 'special session of United Nations']\n",
      "\n",
      "\n",
      "\n",
      " The two United Nations Development Decades, one of the 1960s and the other of the 1970s, and a series of protracted negotiations, have proved sterile exercises, belying the hopes that had been raised that inequity between nations need not be an inexorable law and that, for reasons as much economic as ethical, the rich should assist the poor -> ['series of protracted negotiations', 'inequity between nations']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in df2['Sent'].sample(10, random_state=19).values:\n",
    "    rule = rule3_mod(sent)\n",
    "    if rule != []:\n",
    "        print(sent, '->', rule)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
