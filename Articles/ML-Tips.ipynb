{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Using Stratified K-fold for regression problem**\n",
    "\n",
    "To use stratified k-fold for a regression problem, we have first to divide the target  into bins, and then we can use stratified k-fold in the same way as for classification  problems. There are several choices for selecting the appropriate number of bins. If  you have a lot of samples( > 10k, > 100k), then you don’t need to care about the  number of bins. Just divide the data into 10 or 20 bins. If you do not have a lot of  samples, you can use a simple rule like Sturge’s Rule to calculate the appropriate  number of bins. \n",
    "```\n",
    "Sturge’s rule:  \n",
    "Number of Bins = 1 + log2(N) \n",
    "Where N is the number of samples you have in your dataset.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Feature selection**\n",
    "\n",
    "The simplest form of selecting features would be to remove features with very  low variance. If the features have a very low variance (i.e. very close to 0), they  are close to being constant and thus, do not add any value to any model at all. It  would just be nice to get rid of them and hence lower the complexity. Please note  that the variance also depends on scaling of the data. Scikit-learn has an  implementation for VarianceThreshold that does precisely this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T05:24:57.932778Z",
     "start_time": "2020-08-22T05:24:57.855386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "#fetch a regression dataset  \n",
    "data = fetch_california_housing()  \n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "df = pd.DataFrame(X, columns=col_names) \n",
    "df.loc[:, \"MedInc_Sqrt\"] = df.MedInc.apply(np.sqrt)\n",
    "y = data[\"target\"]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T05:25:09.641682Z",
     "start_time": "2020-08-22T05:25:09.626722Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold  \n",
    "var_thresh = VarianceThreshold(threshold=0.1)  \n",
    "transformed_data = var_thresh.fit_transform(df)  \n",
    "#transformed data will have all columns with variance less  \n",
    "#than 0.1 removed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove features which have a high correlation. For calculating the  correlation between different numerical features, you can use the Pearson  correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T05:25:51.157875Z",
     "start_time": "2020-08-22T05:25:50.858230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedInc_Sqrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MedInc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119034</td>\n",
       "      <td>0.326895</td>\n",
       "      <td>-0.062040</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.018766</td>\n",
       "      <td>-0.079809</td>\n",
       "      <td>-0.015176</td>\n",
       "      <td>0.984329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HouseAge</th>\n",
       "      <td>-0.119034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.153277</td>\n",
       "      <td>-0.077747</td>\n",
       "      <td>-0.296244</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>-0.108197</td>\n",
       "      <td>-0.132797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveRooms</th>\n",
       "      <td>0.326895</td>\n",
       "      <td>-0.153277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847621</td>\n",
       "      <td>-0.072213</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>0.106389</td>\n",
       "      <td>-0.027540</td>\n",
       "      <td>0.326688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveBedrms</th>\n",
       "      <td>-0.062040</td>\n",
       "      <td>-0.077747</td>\n",
       "      <td>0.847621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.066197</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>-0.066910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population</th>\n",
       "      <td>0.004834</td>\n",
       "      <td>-0.296244</td>\n",
       "      <td>-0.072213</td>\n",
       "      <td>-0.066197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069863</td>\n",
       "      <td>-0.108785</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.018415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveOccup</th>\n",
       "      <td>0.018766</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>0.069863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.015266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <td>-0.079809</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.106389</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>-0.108785</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.924664</td>\n",
       "      <td>-0.084303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <td>-0.015176</td>\n",
       "      <td>-0.108197</td>\n",
       "      <td>-0.027540</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>-0.924664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedInc_Sqrt</th>\n",
       "      <td>0.984329</td>\n",
       "      <td>-0.132797</td>\n",
       "      <td>0.326688</td>\n",
       "      <td>-0.066910</td>\n",
       "      <td>0.018415</td>\n",
       "      <td>0.015266</td>\n",
       "      <td>-0.084303</td>\n",
       "      <td>-0.015569</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
       "MedInc       1.000000 -0.119034  0.326895  -0.062040    0.004834  0.018766   \n",
       "HouseAge    -0.119034  1.000000 -0.153277  -0.077747   -0.296244  0.013191   \n",
       "AveRooms     0.326895 -0.153277  1.000000   0.847621   -0.072213 -0.004852   \n",
       "AveBedrms   -0.062040 -0.077747  0.847621   1.000000   -0.066197 -0.006181   \n",
       "Population   0.004834 -0.296244 -0.072213  -0.066197    1.000000  0.069863   \n",
       "AveOccup     0.018766  0.013191 -0.004852  -0.006181    0.069863  1.000000   \n",
       "Latitude    -0.079809  0.011173  0.106389   0.069721   -0.108785  0.002366   \n",
       "Longitude   -0.015176 -0.108197 -0.027540   0.013344    0.099773  0.002476   \n",
       "MedInc_Sqrt  0.984329 -0.132797  0.326688  -0.066910    0.018415  0.015266   \n",
       "\n",
       "             Latitude  Longitude  MedInc_Sqrt  \n",
       "MedInc      -0.079809  -0.015176     0.984329  \n",
       "HouseAge     0.011173  -0.108197    -0.132797  \n",
       "AveRooms     0.106389  -0.027540     0.326688  \n",
       "AveBedrms    0.069721   0.013344    -0.066910  \n",
       "Population  -0.108785   0.099773     0.018415  \n",
       "AveOccup     0.002366   0.002476     0.015266  \n",
       "Latitude     1.000000  -0.924664    -0.084303  \n",
       "Longitude   -0.924664   1.000000    -0.015569  \n",
       "MedInc_Sqrt -0.084303  -0.015569     1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the feature MedInc_Sqrt has a very high correlation with MedInc. We  can thus remove one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Univariate Feature Selection**\n",
    "\n",
    "Univariate  feature selection is nothing but a scoring of each feature against a given target.  Mutual information, ANOVA F-test and chi2 are some of the most popular  methods for univariate feature selection. There are two ways of using these in scikitlearn.  - SelectKBest: It keeps the top-k scoring features  - SelectPercentile: It keeps the top features which are in a percentage  specified by the user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T05:29:38.616679Z",
     "start_time": "2020-08-22T05:29:38.610695Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif  \n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression  \n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T05:38:38.548039Z",
     "start_time": "2020-08-22T05:38:38.529119Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnivariateFeatureSelction:\n",
    "    def __init__(self, n_features, problem_type, scoring):\n",
    "        \"\"\"  \n",
    "        Custom univariate feature selection wrapper on\n",
    "        different univariate feature selection models from  scikit-learn.\n",
    "        :param n_features: SelectPercentile if float else SelectKBest  \n",
    "        :param problem_type: classification or regression  \n",
    "        :param scoring: scoring function, string \n",
    "        \"\"\"\n",
    "    \n",
    "        if problem_type == \"classification\":  \n",
    "            valid_scoring = {\n",
    "                \"f_classif\": f_classif,\n",
    "                \"chi2\": chi2,  \"mutual_info_classif\": mutual_info_classif  \n",
    "            }  \n",
    "        else:  \n",
    "            valid_scoring = {\n",
    "                \"f_regression\": f_regression,\n",
    "                \"mutual_info_regression\": mutual_info_regression  \n",
    "            } \n",
    "            \n",
    "        #raise exception if we do not have a valid scoring method  \n",
    "        if scoring not in valid_scoring:  \n",
    "            raise Exception(\"Invalid scoring function\") \n",
    "            \n",
    "        #if n_features is int, we use selectkbest \n",
    "        #if n_features is float, we use selectpercentile \n",
    "        #please note that it is int in both cases in sklearn  \n",
    "        if isinstance(n_features, int):  \n",
    "            self.selection = SelectKBest(valid_scoring[scoring], k=n_features)\n",
    "        elif isinstance(n_features, float):  \n",
    "            self.selection = SelectPercentile(valid_scoring[scoring],  percentile=int(n_features * 100))\n",
    "        else:\n",
    "            raise Exception(\"Invalid type of feature\")\n",
    "            \n",
    "    #same fit function  \n",
    "    def fit(self, X, y):\n",
    "        return self.selection.fit(X, y)\n",
    "    \n",
    "    #same transform function  \n",
    "    def transform(self, X): \n",
    "        return self.selection.transform(X)  \n",
    "    \n",
    "    #same fit_transform function  \n",
    "    def fit_transform(self, X, y):  \n",
    "        return self.selection.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T05:39:02.175836Z",
     "start_time": "2020-08-22T05:39:01.275790Z"
    }
   },
   "outputs": [],
   "source": [
    "ufs = UnivariateFeatureSelction(\n",
    "    n_features=0.1,\n",
    "    problem_type=\"regression\",  \n",
    "    scoring=\"f_regression\"  \n",
    ")  \n",
    "ufs.fit(X, y) \n",
    "X_transformed = ufs.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Greedy Feature Selection**\n",
    "\n",
    "Univariate feature selection may not always perform  well. Most of the time, people prefer doing feature selection using a machine  learning model.\n",
    "\n",
    "The simplest form of feature selection that uses a model for selection is known as  greedy feature selection. In greedy feature selection, the first step is to choose a  model. The second step is to select a loss/scoring function. And the third and final  step is to iteratively evaluate each feature and add it to the list of **good** features if it improves loss/score. It can’t get simpler than this. But you must keep in mind that  this is known as greedy feature selection for a reason. This feature selection process  will fit a given model each time it evaluates a feature. The computational cost  associated with this kind of method is very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T05:44:27.886813Z",
     "start_time": "2020-08-22T05:44:27.881820Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T05:52:16.651087Z",
     "start_time": "2020-08-22T05:52:16.627148Z"
    }
   },
   "outputs": [],
   "source": [
    "class GreedyFeatureSelection: \n",
    "    \"\"\"  \n",
    "    A simple and custom class for greedy feature selection.  \n",
    "    You will need to modify it quite a bit to make it suitable  \n",
    "    for your dataset. \n",
    "    \"\"\"\n",
    "    def evaluate_score(self, X, y):  \n",
    "        \"\"\"  \n",
    "        This function evaluates model on data and returns  \n",
    "        Area Under ROC Curve (AUC)\n",
    "        NOTE: We fit the data and calculate AUC on same data. \n",
    "        WE ARE OVERFITTING HERE.  \n",
    "        But this is also a way to achieve greedy selection.  \n",
    "        k-fold will take k times longer.   \n",
    "        :param X: training data  \n",
    "        :param y: targets  \n",
    "        :return: overfitted area under the roc curve  \n",
    "        \"\"\"  \n",
    "        #fit the logistic regression model,  \n",
    "        #and calculate AUC on same data  \n",
    "        #you can choose any model that suits your data \n",
    "        model = linear_model.LogisticRegression()  \n",
    "        model.fit(X, y)  \n",
    "        predictions = model.predict_proba(X)[:, 1]  \n",
    "        auc = metrics.roc_auc_score(y, predictions)  \n",
    "        return auc \n",
    "    \n",
    "    def _feature_selection(self, X, y): \n",
    "        \"\"\"  This function does the actual greedy selection  \n",
    "        :param X: data, numpy array  \n",
    "        :param y: targets, numpy array  \n",
    "        :return: (best scores, best features)  \n",
    "        \"\"\" \n",
    "        #initialize good features list  \n",
    "        #and best scores to keep track of both  \n",
    "        \n",
    "        good_features = []  \n",
    "        best_scores = []  \n",
    "        \n",
    "        #calculate the number of features  \n",
    "        num_features = X.shape[1] \n",
    "\n",
    "        #infinite loop  \n",
    "        while True:  \n",
    "\n",
    "            #initialize best feature and score of this loop  \n",
    "            this_feature = None  \n",
    "            best_score = 0 \n",
    "\n",
    "            #loop over all features  \n",
    "            for feature in range(num_features):  \n",
    "                #if feature is already in good features,  \n",
    "                #skip this for loop  \n",
    "                if feature in good_features: \n",
    "                    continue  \n",
    "                    \n",
    "                #selected features are all good features till now  \n",
    "                #and current feature  \n",
    "                selected_features = good_features + [feature]  \n",
    "                \n",
    "                #remove all other features from data  \n",
    "                xtrain = X[:, selected_features]  \n",
    "                \n",
    "                #calculate the score, in our case, AUC  \n",
    "                score = self.evaluate_score(xtrain, y)  \n",
    "                \n",
    "                #if score is greater than the best score  \n",
    "                #of this loop, change best score and best feature  \n",
    "                if score > best_score:  \n",
    "                    this_feature = feature  \n",
    "                    best_score = score \n",
    "                    \n",
    "            #if we have selected a feature, add it \n",
    "            #to the good feature list and update best scores list  \n",
    "            if this_feature != None:  \n",
    "                good_features.append(this_feature)  \n",
    "                best_scores.append(best_score)  \n",
    "                \n",
    "            #if we didnt improve during the last two rounds,  \n",
    "            #exit the while loop  \n",
    "            if len(best_scores) > 2:  \n",
    "                if best_scores[-1] < best_scores[-2]: \n",
    "                    break  \n",
    "                    \n",
    "        #return best scores and good features  \n",
    "        #why do we remove the last data point?  \n",
    "        return best_scores[:-1], good_features[:-1] \n",
    "\n",
    "    def __call__(self, X, y): \n",
    "        \"\"\"  \n",
    "        Call function will call the class on a set of arguments \n",
    "        \"\"\"  \n",
    "        #select features, return scores and selected indices  \n",
    "        scores, features = self._feature_selection(X, y)  \n",
    "        \n",
    "        #transform data with selected features  \n",
    "        return X[:, features], scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T05:52:59.417354Z",
     "start_time": "2020-08-22T05:52:25.988238Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=100)  \n",
    "#transform data by greedy feature selection  \n",
    "X_transformed, scores = GreedyFeatureSelection()(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another greedy approach is known as recursive feature elimination (RFE). In the  previous method, we started with one feature and kept adding new features, but in  RFE, we start with all features and keep removing one feature in every iteration that  provides the least value to a given model. But how to do we know which feature  offers the least value? Well, if we use models like linear support vector machine  (SVM) or logistic regression, we get a coefficient for each feature which decides  the importance of the features. In case of any tree-based models, we get feature  importance in place of coefficients. In each iteration, we can eliminate the least important feature and keep eliminating it until we reach the number of features  needed. So, yes, we have the ability to decide how many features we want to keep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T05:56:02.594996Z",
     "start_time": "2020-08-22T05:56:01.699651Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "from sklearn.feature_selection import RFE  \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.datasets import fetch_california_housing \n",
    "\n",
    "#fetch a regression dataset  \n",
    "data = fetch_california_housing()  \n",
    "X = data[\"data\"]  \n",
    "col_names = data[\"feature_names\"] \n",
    "y = data[\"target\"] \n",
    "\n",
    "#initialize the model  \n",
    "model = LinearRegression()  \n",
    "#initialize RFE  \n",
    "rfe = RFE(  \n",
    "    estimator=model,  \n",
    "    n_features_to_select=3  \n",
    ")  \n",
    "\n",
    "#fit RFE  \n",
    "rfe.fit(X, y)  \n",
    "\n",
    "#get the transformed data with  \n",
    "#selected columns  \n",
    "X_transformed = rfe.transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
